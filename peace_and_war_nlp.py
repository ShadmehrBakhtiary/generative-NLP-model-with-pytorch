# -*- coding: utf-8 -*-
"""peace-and-war-nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LMht5rUGNEI3Ajtx6JqtkB2Rqx4hpzh2
"""

from google.colab import drive
drive.mount('/content/drive')

"""import liabraries"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
import torch.nn.functional as F

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

with open('/content/drive/MyDrive/ML_FOLDER/ai-project/nlp-generative-novel/war_and_peace.txt','r',encoding='utf8') as f:
    text = f.read()

print(text[:2000])

len(text)

"""# **Encode Entire Text**"""

all_characters = set(text)
decoder = dict(enumerate(all_characters))
decoder.items()

encoder = {char: ind for ind,char in decoder.items()}
encoded_text = np.array([encoder[char] for char in text])
encoded_text[:500]

"""# **One Hot Encoding**"""

def one_hot_encoder(encoded_text, num_uni_chars):
    one_hot = np.zeros((encoded_text.size, num_uni_chars))

    # Convert data type for later use with pytorch (errors if we dont!)
    one_hot = one_hot.astype(np.float32)

    # Using indexing fill in the 1s at the correct index locations
    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0


    # Reshape it so it matches the batch sahe
    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))

    return one_hot

one_hot_encoder(np.array([1,2,0]),3)

"""# **Creating Training Batches**"""

def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):


    char_per_batch = samp_per_batch * seq_len


    num_batches_avail = int(len(encoded_text)/char_per_batch)

    encoded_text = encoded_text[:num_batches_avail * char_per_batch]

    # Reshape text into rows the size of a batch
    encoded_text = encoded_text.reshape((samp_per_batch, -1))

    for n in range(0, encoded_text.shape[1], seq_len):


        x = encoded_text[:, n:n+seq_len]


        y = np.zeros_like(x)


        try:
            y[:, :-1] = x[:, 1:]
            y[:, -1]  = encoded_text[:, n+seq_len]

        # FOR POTENTIAL INDEXING ERROR AT THE END
        except:
            y[:, :-1] = x[:, 1:]
            y[:, -1] = encoded_text[:, 0]

        yield x, y

sample_text = encoded_text[:20]
sample_text

batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)
x, y = next(batch_generator)

x

y

torch.cuda.is_available()

"""# **Creating the LSTM Model**"""

class CharModel(nn.Module):

    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=True):


        # SET UP ATTRIBUTES
        super().__init__()
        self.drop_prob = drop_prob
        self.num_layers = num_layers
        self.num_hidden = num_hidden
        self.use_gpu = use_gpu

        #CHARACTER SET, ENCODER, and DECODER
        self.all_chars = all_chars
        self.decoder = dict(enumerate(all_chars))
        self.encoder = {char: ind for ind,char in decoder.items()}


        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)

        self.dropout = nn.Dropout(drop_prob)

        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))


    def forward(self, x, hidden):


        lstm_output, hidden = self.lstm(x, hidden)


        drop_output = self.dropout(lstm_output)

        drop_output = drop_output.contiguous().view(-1, self.num_hidden)


        final_out = self.fc_linear(drop_output)


        return final_out, hidden


    def hidden_state(self, batch_size):
        '''
        Used as separate method to account for both GPU and CPU users.
        '''

        if self.use_gpu:

            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),
                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())
        else:
            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),
                     torch.zeros(self.num_layers,batch_size,self.num_hidden))

        return hidden

model = CharModel(
    all_chars=all_characters,
    num_hidden=512,
    num_layers=3,
    drop_prob=0.5,
    use_gpu=True,
)
total_param  = []
for p in model.parameters():
    total_param.append(int(p.numel()))

len(encoded_text)

"""# **training Data and Validation Data**"""

optimizer = torch.optim.Adam(model.parameters(),lr=0.001)
criterion = nn.CrossEntropyLoss()
train_percent = 0.1
train_ind = int(len(encoded_text) * (train_percent))
train_data = encoded_text[:train_ind]
val_data = encoded_text[train_ind:]

epochs = 50
# batch size
batch_size = 128

# Length of sequence
seq_len = 100

# for printing report purposes
# always start at 0
tracker = 0

# number of characters in text
num_char = max(encoded_text)+1

model.train()


if model.use_gpu:
    model.cuda()

for i in range(epochs):

    hidden = model.hidden_state(batch_size)


    for x,y in generate_batches(train_data,batch_size,seq_len):

        tracker += 1

        # One Hot Encode incoming data
        x = one_hot_encoder(x,num_char)

        # Convert Numpy Arrays to Tensor

        inputs = torch.from_numpy(x)
        targets = torch.from_numpy(y)


        if model.use_gpu:

            inputs = inputs.cuda()
            targets = targets.cuda()

        # Reset Hidden State

        hidden = tuple([state.data for state in hidden])

        model.zero_grad()

        lstm_output, hidden = model.forward(inputs,hidden)
        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())

        loss.backward()


        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)

        optimizer.step()

        if tracker % 25 == 0:

            val_hidden = model.hidden_state(batch_size)
            val_losses = []
            model.eval()

            for x,y in generate_batches(val_data,batch_size,seq_len):

                # One Hot Encode incoming data
                x = one_hot_encoder(x,num_char)


                # Convert Numpy Arrays to Tensor

                inputs = torch.from_numpy(x)
                targets = torch.from_numpy(y)

                # Adjust for GPU if necessary

                if model.use_gpu:

                    inputs = inputs.cuda()
                    targets = targets.cuda()

                # Reset Hidden State

                val_hidden = tuple([state.data for state in val_hidden])

                lstm_output, val_hidden = model.forward(inputs,val_hidden)
                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())

                val_losses.append(val_loss.item())

            # Reset to training model after val for loop
            model.train()

            print(f"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}")

model_name = 'example.net'
torch.save(model.state_dict(),model_name)

"""## Load Model"""

model = CharModel(
    all_chars=all_characters,
    num_hidden=512,
    num_layers=3,
    drop_prob=0.5,
    use_gpu=True,
)

model.load_state_dict(torch.load(model_name))
model.eval()

"""generate prediction"""

def predict_next_char(model, char, hidden=None, k=1):


        encoded_text = model.encoder[char]

        encoded_text = np.array([[encoded_text]])

        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))

        inputs = torch.from_numpy(encoded_text)

        # Check for CPU
        if(model.use_gpu):
            inputs = inputs.cuda()


        hidden = tuple([state.data for state in hidden])


        lstm_out, hidden = model(inputs, hidden)


        probs = F.softmax(lstm_out, dim=1).data


        if(model.use_gpu):
            # move back to CPU to use with numpy
            probs = probs.cpu()


        probs, index_positions = probs.topk(k)


        index_positions = index_positions.numpy().squeeze()

        probs = probs.numpy().flatten()


        probs = probs/probs.sum()

        char = np.random.choice(index_positions, p=probs)

        # return the encoded value of the predicted char and the hidden state
        return model.decoder[char], hidden

def generate_text(model, size, seed='The', k=1):



    # CHECK FOR GPU
    if(model.use_gpu):
        model.cuda()
    else:
        model.cpu()

    # Evaluation mode
    model.eval()

    # begin output from initial seed
    output_chars = [c for c in seed]

    # intiate hidden state
    hidden = model.hidden_state(1)

    # predict the next character for every character in seed
    for char in seed:
        char, hidden = predict_next_char(model, char, hidden, k=k)

    # add initial characters to output
    output_chars.append(char)

    # Now generate for size requested
    for i in range(size):

        # predict based off very last letter in output_chars
        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)

        # add predicted character
        output_chars.append(char)

    # return string of predicted text
    return ''.join(output_chars)

print(generate_text(model, 1000, seed='Buonaparte ', k=3))